---
date: 2017-12-01
title: Multi-agents Reinforcement Learning
tags : ["IA","RL","Markov"]
---
# Understand Multi Agent Reinforcement learning (MARL) : a state of the art method

***

Hi ! In this post I will explain to you one of the most efficient method for multi-agent reinforcement learning. This method is derived from paper : *Grid-Wise Control for Multi-Agent Reinforcement Learning in Video Game AI*.

Before reading this I advise the reader to have some notion of learning by reinforcement, deep learning, markov process, and optimisation.

Let's go into a little more detail! :smiley:

## A quick overview of ways of looking at the MARL problem

***

When you're looking to train multiple agents to solve a task, you need to ask yourself how you're going to pose the problem.

- Will I consider all my agents as a single agent, i.e. have a global policy, status and rewards for all my agents  
> It's what we  called **centralized learning**, centralized learning is a kind "change of variables" where we consider that our agent is the union of all our agents. This way of looking at the problem allows us to **maximize the coordination** of our agents. But do you see the problem in this way? Imagine that we have $N$ agents with each $K$ action then the cardinality of the action space will be $K^N$ action. In sum the join-action space is to large and a slight increase in the number of agents would increase the size of this space exponentially.





- Or will I consider all my agents independently ?
> It's what we  called **decentralized learning** ,in decentralized learning each agents learn his own policy based on his "local observation-action trajectory" (see [Independant Q-learning](http://web.media.mit.edu/~cynthiab/Readings/tan-MAS-reinfLearn.pdf)).As you can imagine, this type of learning has difficulty modeling communication between agents.


Of course there are methods between these two visions which are a mixture of centralized and decentralized learning. *Nothing's all black and white, it's the gray that wins.*
This article does not aim to explain all the learning methods (you can find references to methods (centralized, decentralized and mix in the original paper).

- The important thing to remember is that these solutions have one major flaw :
>  "For many multi-agent settings, the **number of agents acting
in the environment keeps changing both within and across
episodes**. For example, in video games, the agents may die
or be out of control while new agents may join in, e.g., the
battle game in StraCraft. Similarly, in real-world traffic,
vehicles enter and exit the traffic network over time, inducing complex dynamics. Therefore, a main challenge is to
**flexibly control an arbitrary number of agents and achieve
effective collaboration at the same time**. Unfortunately, all
the aforementioned MARL methods suffer from **trading-off
between centralized and decentralized learning to leverage
agent communication and individual flexibility**. Actually,
most of the existing MARL algorithms make a default assumption that the number of agents is **fixed before learning**.
Many of them adopt a well designed reinforcement learning
structure, which, however, depends on the number of agents."

## A solution: The Grid-Wise Control

***

Let's get to the heart of the matter!

We will take as an example the optimization of a surveillance network (fixed camera, mobile camera) to explain the architecture : *How best to use the different sensors in an area to track a target ?*


To bring a solution to the MARL problem we define a well known architecture in deep learning: an **encoder-decoder :hourglass_flowing_sand:** .

We will present this architecture layer by layer in a rather static way. Then we will see how it works and the different algorithms it uses to train the agents.

{{< figure library="true" src="GW_archi.png" title="grid-wise control architecture" lightbox="true" >}}


#### Input layer :


* **Input tensor** : The state grid $s \in \mathbb{R}^{w \times h \times c_{s}}$ where $w$ is the width of the grid, $h$ is the height and $c_{s}$ the features number. We can see this tensor as a stack of $c_s$ feature maps on our agents and our environment.

Let's take an example: Let's imagine that we want to train a set of sensors to monitor a city.

{{< figure library="true" src="cam.jpg" title="City" lightbox="true" >}}


Our architecture can't take a satellite image of the city. We have to find a way to encode the relevant information of the city and the agents in the form of a set of grids (a tensor). For example, we can give a grid that shows the route and the current position of the cameras :

{{< figure library="true" src="grid_ex.png" title="Feature-map example : R = road and C = camera" lightbox="true" >}}


This way we can create full feature tensors that represent the current state of our agents in the environment. We can then give this tensor to our network.

#### Encoder block :

* **Convolutional encoder** : The state $s$ (our features tensor, a stack of feature cards) is fed to a convolutional network.
